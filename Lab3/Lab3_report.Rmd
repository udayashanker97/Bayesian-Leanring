---
  title: "Lab Report:Lab3-Group8"
  author: "Udaya Shanker Mohanan Nair(udamo524), Uday Jain(udaja983)"
  output:
    pdf_document:
      latex_engine: xelatex
  date: "`r Sys.Date()`"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(size = "small")
```

## Assignment 1 Gibbs sampling for the logistic regression

```{r, echo=TRUE, eval=TRUE}
##ASSIGNMENT 1

library(mvtnorm)
library(BayesLogit)

disease_data <- read.csv("Disease.csv")
x <- cbind(
  Intercept = 1,
  age_new = (disease_data$age - 
               mean(disease_data$age))/sd(disease_data$age),
  gender = disease_data$gender,
  duration_new = (disease_data$duration_of_symptoms -
                    mean(disease_data$duration_of_symptoms))/sd(disease_data$duration_of_symptoms),
  dyspnoea = disease_data$dyspnoea,
  white_Blood_new = (disease_data$white_blood - 
                       mean(disease_data$white_blood))/sd(disease_data$white_blood)
)

y <- disease_data$class_of_diagnosis
size <- nrow(x)
col_size <- ncol(x)

tau <- 3


gibbs_implementation <- function(x, y, n_iter=1000) {
  beta_samples <- matrix(0, n_iter, ncol(x))
  
  beta_value <- rep(0, ncol(x))
  size <- nrow(x)
  col_size <- ncol(x)
  
  prior_precision <- diag(1/tau^2, col_size)
  
  for (iter in 1:n_iter) {
    z <- x %*% beta_value
    poly_gamma <- rpg(size, 1, z)
    v <- solve(t(x) %*% diag(poly_gamma) %*% x + prior_precision)
    m <- v %*% t(x) %*% (y - 0.5)
    beta_value <- as.numeric(rmvnorm(1, m, v))
    beta_samples[iter,] <- beta_value
  }
  return(beta_samples)
}

set.seed(123)
results <- gibbs_implementation(x, y)

if_evaluation <- function(beta_values) {
  acf_value <- acf(beta_values, plot=FALSE, lag.max=50)$acf[,,1]
  if_value <- 1 + 2 * sum(acf_value[-1])
  return(if_value)
}

res_if <- apply(results, 2, if_evaluation)
cat("Inefficiency Factors for whole dataset:\n")
print(res_if)

plot(1, type = "n", 
     xlim = c(1, nrow(results)), 
     ylim = range(results),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler for whole dataset")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

results_for_10_observations <- gibbs_implementation(x[1:10,], y[1:10])
results_for_40_observations <- gibbs_implementation(x[1:40,], y[1:40])
results_for_80_observations <- gibbs_implementation(x[1:80,], y[1:80])

res_if_10_observations <- apply(results_for_10_observations, 2, if_evaluation)
cat("Inefficiency Factors for 10 Observations:\n")
print(res_if_10_observations)

res_if_40_observations <- apply(results_for_40_observations, 2, if_evaluation)
cat("Inefficiency Factors for 40 Observations:\n")
print(res_if_40_observations)

res_if_80_observations <- apply(results_for_80_observations, 2, if_evaluation)
cat("Inefficiency Factors for 80 Observations:\n")
print(res_if_80_observations)

par(mfrow = c(1,3))
#10 Observations
plot(1, type = "n", 
     xlim = c(1, nrow(results_for_10_observations)), 
     ylim = range(results_for_10_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_10_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8) 

#40 Observations

plot(1, type = "n", 
     xlim = c(1, nrow(results_for_40_observations)), 
     ylim = range(results_for_40_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_40_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

#80 Observations
plot(1, type = "n", 
     xlim = c(1, nrow(results_for_80_observations)), 
     ylim = range(results_for_80_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_80_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

par(mfrow = c(1,1))

```

## Assignment 2 : Metropolis Random Walk for Poisson regression

```{r, echo=TRUE, eval=TRUE}
##ASSIGNMENT 2
library(mvtnorm)
library(MASS)
data<-read.table("eBayNumberOfBidderData_2025.dat",header=TRUE)
X <- model.matrix(~ PowerSeller + VerifyID + Sealed + 
                    Minblem + MajBlem + LargNeg + LogBook + 
                    MinBidShare, data = data)
y <- data$nBids
################################### Part A ############################
poisson_glm<-glm(y ~ . -1,data=as.data.frame(X),family=poisson)
cat("Summary\n")
print(summary(poisson_glm))
cat("\nAs observed from summary, Intercept along with VerifyID, Sealed, \n 
MajBlem, LogBook and MinBidShare are significant covariates.")
################################### Part B ############################
# Log-posterior function (with Zellner's g-prior)
log_posterior <- function(beta, X, y) {
  lambda <- exp(X %*% beta)       # exp(XÎ²) = predicted Poisson rates
  log_lik <- sum(dpois(y,lambda,log=TRUE))    # Sum of log-likelihoods
  log_prior <- dmvnorm(beta,mean=rep(0,dim(X)[2]), 
                       sigma=100*solve(t(X)%*%X),log=TRUE)
  return(log_lik + log_prior)
}
# Find posterior mode
optim_result<-optim(rep(0,dim(X)[2]),log_posterior,X=X,y=y, 
                      control=list(fnscale=-1),hessian=TRUE)
beta_tilde<-optim_result$par
J_inv<-solve(-optim_result$hessian)  # Posterior covariance
names(beta_tilde)<-colnames(X)
colnames(J_inv)<-rownames(J_inv)<-colnames(X)
cat("\nBeta_tilde:\n")
print(beta_tilde)
cat("\nCovariance Matrix (Jy(-1) beta_tilde)\n")
print(J_inv)
################################### Part C ############################
RWMSampler <- function(logPostFunc,theta_init,cov_prop,n_iter,burn_in,c,...) {
  # logPostFunc: Function to compute the log-posterior density. First argument 
  # must be `theta`.
  # theta_init: Initial parameter vector.
  # cov_prop Proposal covariance matrix.
  # n_iter Total number of iterations.
  # burn_in Burn-in samples.
  # c Step size scaling factor.  # Tune for 25-30% acceptance
  p<-length(theta_init)
  theta<-matrix(NA, n_iter, p)
  theta[1, ]<-theta_init
  log_post_current<-logPostFunc(theta[1, ], ...)
  n_accept<-0
  for (i in 2:n_iter) {
    # Propose new theta
    theta_prop<-MASS::mvrnorm(1,mu=theta[i-1, ],Sigma=c*cov_prop)
    # Compute log-posterior at proposal
    # Log acceptance probability (avoid numerical overflow)
    log_alpha<-logPostFunc(theta_prop,...)-logPostFunc(theta[i-1, ], ...)
    if(log(runif(1)) < log_alpha){
      theta[i, ]<-theta_prop
      n_accept<-n_accept+1
    } else{
      theta[i, ]<-theta[i-1, ]
    }
  }
  theta<-theta[(burn_in+1):n_iter, ]
  acceptance_rate<-n_accept/n_iter
  cat("Acceptance rate:", acceptance_rate,"\n")
  return(theta)
}

LogPostPoisson <- function(theta, X, y) {
  lambda<-exp(X%*%theta)
  log_lik<-sum(dpois(y,lambda,log=TRUE))
  log_prior<-mvtnorm::dmvnorm(theta,mean=rep(0,ncol(X)),
                              sigma=100*solve(t(X)%*%X),log=TRUE)
  return(log_lik+log_prior)
}
samplesMH<-RWMSampler(logPostFunc=LogPostPoisson,theta_init=beta_tilde,
                     cov_prop=J_inv,n_iter=10000,burn_in=500,c=0.7,X=X,y=y)
colnames(samplesMH)<-colnames(X)
plot.ts(samplesMH, main = "Trace Plot for MH estimated Beta(s)")
cat("\nSummary Results\n")
print(data.frame(Covariate=colnames(samplesMH),
           Mean=round(colMeans(samplesMH),4),
           Std=round(apply(samplesMH,2,sd),4)),row.names=FALSE)
cat("\nComparing the values with those in part B, we find while values for
    Intercept, Sealed , LogBook and MinBidShare are similar, other covariates
    show significant differences in values\n")

################################### Part D ############################
x_new<-c(1,1,0,1,0,1,0,1.3,0.7)
lambda_new<-exp(samplesMH %*% x_new)
y_new<-rpois(nrow(samplesMH),lambda_new)
hist(y_new,breaks=30,main="Predictive Distribution for Number of Bidders")
cat("\nProbability of no bidders (y_new = 0):",mean(y_new==0),"\n")

```

## Assignment 3 : Time series models in Stan

```{r, echo=TRUE, eval=TRUE}
library(rstan)
library(ggplot2) 
#Part A
ar1_simulation <- function(mu, phi, sigma_square, T) {
  results <- numeric(T)
  results[1] <- mu
  for (value in 2:T) {
    results[value] <- mu + phi * (results[value - 1] - mu) +
      rnorm(1, mean = 0, sd = sqrt(sigma_square))
  }
  return(results)
}

set.seed(123)
mu <- 5
sigma_square <- 9
T <- 300
phi_values <- c(-0.5, 0.1, 0.5)

par(mfrow = c(3,1))
for (phi_value in phi_values) {
  result <- ar1_simulation(mu, phi_value, sigma_square, T)
  plot(result, type = 'l', main = paste("AR(1) for phi =", phi_value),
       xlab = "Time (t)", ylab = "Value")
}
par(mfrow = c(1,1))
```

for phi = -0.5, it shows an alternative behaviour this is because of negative
autocorrelation.Tendency of the values to move in opposite direction from the
previous value.

Now for phi = 0.1, when you compare nearby values, in most of the case current
value doesn't much depend on the previous value. So therefore it is having weak
autocorrelation.

Now for phi = 0.5,values are showing some tendency to change smoothly as the 
time move forward. so in a way it can be considered as a moderate positive 
autocorrelation.

```{r, echo=TRUE, eval=TRUE}
#Part B
x_results_phi4 <- ar1_simulation(mu, 0.4, sigma_square, T)
x_results_phi98 <- ar1_simulation(mu, 0.98, sigma_square, T)

stan_code <- "
  data {
    int<lower=1> T;
    vector[T] results;
  }
  
  parameters {
    real mu;
    real<lower=-1, upper=1> phi;
    real<lower=0> sigma;
  }
  
  model {
    results[1] ~ normal(mu, sigma);
    for (t in 2:T) {
      results[t] ~ normal(mu + phi * (results[t - 1] - mu), sigma);
    }
  }
"

stan_model <- stan_model(model_code = stan_code)

stan_code_data4 <- list(T = T, results = x_results_phi4)
stan_code_data98 <- list(T = T, results = x_results_phi98)

fit_phi4 <- sampling(stan_model, data = stan_code_data4, chains = 5, iter = 5000)

fit_phi98 <- sampling(stan_model, data = stan_code_data98, chains = 5, iter = 5000)

# (i)

cat("\nResults when phi is 0.4:\n")
summary(fit_phi4, pars = c("mu", "phi", "sigma"), probs = c(0.025, 0.975))

cat("\nResults when phi is 0.98:\n")
summary(fit_phi98, pars = c("mu", "phi", "sigma"), probs = c(0.025, 0.975))

```

Results shows that the posterior summaries from stan were able to successfully
estimate the true values of ar1 stimulation with some minor changes of aroung
0.25 difference for each paramter value.

```{r, echo=TRUE, eval=TRUE}
#(ii)

#Convergence of the plots

traceplot(fit_phi4, pars = c("mu", "phi")) +
  ggtitle("Convergence Plot when phi is 0.4")


traceplot(fit_phi98, pars = c("mu", "phi"))+
  ggtitle("Convergence Plot when phi is 0.98")

posterior_phi4 <- as.matrix(fit_phi4)

plot(
  posterior_phi4[, "mu"], posterior_phi4[, "phi"],
  main = "Joint Posterior of mu and phi (phi = 0.4)",
  xlab = expression(mu),
  ylab = expression(phi),
  pch = 20, col = rgb(0, 0, 1, 0.2)
)

posterior_phi98 <- as.matrix(fit_phi98)

plot(
  posterior_phi98[, "mu"], posterior_phi98[, "phi"],
  main = "Joint Posterior of mu and phi (phi = 0.98)",
  xlab = expression(mu),
  ylab = expression(phi),
  pch = 20, col = rgb(1, 0, 0, 0.2)
)

```