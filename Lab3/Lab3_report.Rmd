---
  title: "Lab Report:Lab3-Group8"
  author: "Udaya Shanker Mohanan Nair(udamo524), Uday Jain(udaja983)"
  output:
    pdf_document:
      latex_engine: xelatex
  date: "`r Sys.Date()`"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(size = "small")
```

## Assignment 1 : Gibbs samcol_sizeling for the logistic regression

```{r, echo=TRUE, eval=TRUE}
##ASSIGNMENT 1

library(mvtnorm)
library(BayesLogit)

disease_data <- read.csv("Disease.csv")
x <- cbind(
  Intercept = 1,
  age_new = (disease_data$age - 
               mean(disease_data$age))/sd(disease_data$age),
  gender = disease_data$gender,
  duration_new = (disease_data$duration_of_symptoms -
                    mean(disease_data$duration_of_symptoms))/sd(disease_data$duration_of_symptoms),
  dyspnoea = disease_data$dyspnoea,
  white_Blood_new = (disease_data$white_blood - 
                       mean(disease_data$white_blood))/sd(disease_data$white_blood)
)

y <- disease_data$class_of_diagnosis
size <- nrow(x)
col_size <- ncol(x)

tau <- 3


gibbs_implementation <- function(x, y, n_iter=1000) {
  beta_samples <- matrix(0, n_iter, ncol(x))
  
  beta_value <- rep(0, ncol(x))
  size <- nrow(x)
  col_size <- ncol(x)
  
  prior_precision <- diag(1/tau^2, col_size)
  
  for (iter in 1:n_iter) {
    z <- x %*% beta_value
    poly_gamma <- rpg(size, 1, z)
    v <- solve(t(x) %*% diag(poly_gamma) %*% x + prior_precision)
    m <- v %*% t(x) %*% (y - 0.5)
    beta_value <- as.numeric(rmvnorm(1, m, v))
    beta_samples[iter,] <- beta_value
  }
  return(beta_samples)
}

set.seed(123)
results <- gibbs_implementation(x, y)

if_evaluation <- function(beta_values) {
  acf_value <- acf(beta_values, plot=FALSE, lag.max=50)$acf[,,1]
  if_value <- 1 + 2 * sum(acf_value[-1])
  return(if_value)
}

res_if <- apply(results, 2, if_evaluation)
cat("Inefficiency Factors for whole dataset:\n")
print(res_if)

plot(1, type = "n", 
     xlim = c(1, nrow(results)), 
     ylim = range(results),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler for whole dataset")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

results_for_10_observations <- gibbs_implementation(x[1:10,], y[1:10])
results_for_40_observations <- gibbs_implementation(x[1:40,], y[1:40])
results_for_80_observations <- gibbs_implementation(x[1:80,], y[1:80])

res_if_10_observations <- apply(results_for_10_observations, 2, if_evaluation)
cat("Inefficiency Factors for 10 Observations:\n")
print(res_if_10_observations)

res_if_40_observations <- apply(results_for_40_observations, 2, if_evaluation)
cat("Inefficiency Factors for 40 Observations:\n")
print(res_if_40_observations)

res_if_80_observations <- apply(results_for_80_observations, 2, if_evaluation)
cat("Inefficiency Factors for 80 Observations:\n")
print(res_if_80_observations)

par(mfrow = c(1,3))
#10 Observations
plot(1, type = "n", 
     xlim = c(1, nrow(results_for_10_observations)), 
     ylim = range(results_for_10_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_10_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8) 

#40 Observations

plot(1, type = "n", 
     xlim = c(1, nrow(results_for_40_observations)), 
     ylim = range(results_for_40_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_40_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

#80 Observations
plot(1, type = "n", 
     xlim = c(1, nrow(results_for_80_observations)), 
     ylim = range(results_for_80_observations),
     xlab = "Iteration", 
     ylab = "Value",
     main = "Gibbs Sampler Trajectories - All Parameters")

cols <- rainbow(col_size)

for (i in 1:col_size) {
  lines(results_for_80_observations[, i], col = cols[i], lwd = 1.5)
}
legend("topright", 
       legend = paste0("beta[", 0:(col_size-1), "]"), 
       col = cols, 
       lty = 1, 
       lwd = 1.5,
       cex = 0.8)  

par(mfrow = c(1,1))

```

## Assignment 2 : Metropolis Random Walk for Poisson regression

```{r, echo=TRUE, eval=TRUE}
##ASSIGNMENT 2
library(mvtnorm)
library(MASS)
data<-read.table("eBayNumberOfBidderData_2025.dat",header=TRUE)
X <- model.matrix(~ PowerSeller + VerifyID + Sealed + 
                    Minblem + MajBlem + LargNeg + LogBook + 
                    MinBidShare, data = data)
y <- data$nBids
################################### Part A ############################
poisson_glm<-glm(y ~ . -1,data=as.data.frame(X),family=poisson)
cat("Summary\n")
print(summary(poisson_glm))
cat("\nAs observed from summary, Intercept along with VerifyID, Sealed, \n 
MajBlem, LogBook and MinBidShare are significant covariates.")
################################### Part B ############################
# Log-posterior function (with Zellner's g-prior)
log_posterior <- function(beta, X, y) {
  lambda <- exp(X %*% beta)       # exp(XÎ²) = predicted Poisson rates
  log_lik <- sum(dpois(y,lambda,log=TRUE))    # Sum of log-likelihoods
  log_prior <- dmvnorm(beta,mean=rep(0,dim(X)[2]), 
                       sigma=100*solve(t(X)%*%X),log=TRUE)
  return(log_lik + log_prior)
}
# Find posterior mode
optim_result<-optim(rep(0,dim(X)[2]),log_posterior,X=X,y=y, 
                      control=list(fnscale=-1),hessian=TRUE)
beta_tilde<-optim_result$par
J_inv<-solve(-optim_result$hessian)  # Posterior covariance
names(beta_tilde)<-colnames(X)
colnames(J_inv)<-rownames(J_inv)<-colnames(X)
cat("\nBeta_tilde:\n")
print(beta_tilde)
cat("\nCovariance Matrix (Jy(-1) beta_tilde)\n")
print(J_inv)
################################### Part C ############################
RWMSampler <- function(logPostFunc,theta_init,cov_prop,n_iter,burn_in,c,...) {
  # logPostFunc: Function to compute the log-posterior density. First argument 
  # must be `theta`.
  # theta_init: Initial parameter vector.
  # cov_prop Proposal covariance matrix.
  # n_iter Total number of iterations.
  # burn_in Burn-in samples.
  # c Step size scaling factor.  # Tune for 25-30% acceptance
  p<-length(theta_init)
  theta<-matrix(NA, n_iter, p)
  theta[1, ]<-theta_init
  log_post_current<-logPostFunc(theta[1, ], ...)
  n_accept<-0
  for (i in 2:n_iter) {
    # Propose new theta
    theta_prop<-MASS::mvrnorm(1,mu=theta[i-1, ],Sigma=c*cov_prop)
    # Compute log-posterior at proposal
    # Log acceptance probability (avoid numerical overflow)
    log_alpha<-logPostFunc(theta_prop,...)-logPostFunc(theta[i-1, ], ...)
    if(log(runif(1)) < log_alpha){
      theta[i, ]<-theta_prop
      n_accept<-n_accept+1
    } else{
      theta[i, ]<-theta[i-1, ]
    }
  }
  theta<-theta[(burn_in+1):n_iter, ]
  acceptance_rate<-n_accept/n_iter
  cat("Acceptance rate:", acceptance_rate,"\n")
  return(theta)
}

LogPostPoisson <- function(theta, X, y) {
  lambda<-exp(X%*%theta)
  log_lik<-sum(dpois(y,lambda,log=TRUE))
  log_prior<-mvtnorm::dmvnorm(theta,mean=rep(0,ncol(X)),
                              sigma=100*solve(t(X)%*%X),log=TRUE)
  return(log_lik+log_prior)
}
samplesMH<-RWMSampler(logPostFunc=LogPostPoisson,theta_init=beta_tilde,
                     cov_prop=J_inv,n_iter=10000,burn_in=500,c=0.7,X=X,y=y)
colnames(samplesMH)<-colnames(X)
plot.ts(samplesMH, main = "Trace Plot for MH estimated Beta(s)")
cat("\nSummary Results\n")
print(data.frame(Covariate=colnames(samplesMH),
           Mean=round(colMeans(samplesMH),4),
           Std=round(apply(samplesMH,2,sd),4)),row.names=FALSE)
cat("\nComparing the values with those in part B, we find while values for
    Intercept, Sealed , LogBook and MinBidShare are similar, other covariates
    show significant differences in values\n")

################################### Part D ############################
x_new<-c(1,1,0,1,0,1,0,1.3,0.7)
lambda_new<-exp(samplesMH %*% x_new)
y_new<-rpois(nrow(samplesMH),lambda_new)
hist(y_new,breaks=30,main="Predictive Distribution for Number of Bidders")
cat("\nProbability of no bidders (y_new = 0):",mean(y_new==0),"\n")

```